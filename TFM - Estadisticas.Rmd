---
title: "TFM - Estadisticas"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



******
# Lectura inicial
******

Leemos fichero de datos y seleccionamos columnas necesarias

```{r}

# Para la función Corpus()
library(tm)
# Para la función rbind.fill
library(plyr)
library(SnowballC)
# Para los graficos
library(ggplot2)  
# Para la nube de palabras
library(wordcloud)

#directorio de trabajo
nombreruta <-getwd()

#leemos todos los ficheros con datos de Facebook
fdatos <- data.frame()
file.names <- dir(nombreruta, pattern = "ANSII.txt")
for(i in 1:length(file.names)){
  file <- read.csv(file.names[i], header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII", sep="\t", fill=TRUE)
  fdatos <- rbind(fdatos, file)
}

fsentim <-  read.csv("/Users/Raul/Documents/R/tfm/Sentiment_output_en.csv", header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII", sep=";")
#seleccionamos solo las columnas que queremos para las estadisticas
columnas <- c("type", "likes_count_fb", "comments_count_fb", "reactions_count_fb","shares_count_fb","engagement_fb")
datos <- data.frame(fdatos[,columnas])

datos_sent <- cbind (datos, fsentim)
head(datos_sent,5)
summary(datos_sent)


#transformamos el type a factor

datos_sent$type <- as.factor(datos$type)

```

******
# Estadísticas básicas
******

Vamos a calcular el mínimo, máximo, media y desviación típica de cada uno de los contadores

```{r}
summary(datos_sent$likes_count_fb)
sd(datos_sent$likes_count_fb)

summary(datos_sent$comments_count_fb)
sd(datos_sent$comments_count_fb)

summary(datos_sent$reactions_count_fb)
sd(datos_sent$reactions_count_fb)

summary(datos_sent$shares_count_fb)
sd(datos_sent$shares_count_fb)

summary(datos_sent$engagement_fb)
sd(datos_sent$engagement_fb)

summary(datos_sent$polarity)
sd(datos_sent$polarity)

#En el caso de 'type' al ser una variable categórica lo que calculamos es la frecuencia de cada uno de los factores.
freq_type <- as.data.frame(base::table(datos_sent$type))
freq_type

```


Vamos a calcular las medias en función de la variable type
```{r}

#Calculamos las medias en funcion del type

tapply(datos$likes_count_fb, datos$type, mean)
tapply(datos$comments_count_fb, datos$type, mean)
tapply(datos$reactions_count_fb, datos$type, mean)
tapply(datos$shares_count_fb, datos$type, mean)
tapply(datos$engagement_fb, datos$type, mean)

```

calculamos también las desviaciones típicas

```{r}

#Calculamos las medias en funcion del type

tapply(datos$likes_count_fb, datos$type, sd)
tapply(datos$comments_count_fb, datos$type, sd)
tapply(datos$reactions_count_fb, datos$type, sd)
tapply(datos$shares_count_fb, datos$type, sd)
tapply(datos$engagement_fb, datos$type, sd)

```


******
# Análisis por tipo de contenido
******

Ahora lo hacemos para comparar la variable categórica 'type' con el resto de variables continuas. En este caso usamos el DF sin filtrar, ya que la polaridad no afecta.

```{r}
ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = type, y =   likes_count_fb))+
  xlab("type") + ylab("nº de likes") 

ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = type, y =   comments_count_fb))+
  xlab("type") + ylab("nº de comment") 

ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = type, y =   reactions_count_fb))+
  xlab("type") + ylab("nº de reacciones") 

ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = type, y =   shares_count_fb))+
  xlab("type") + ylab("nº de shares") 

ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = type, y =   engagement_fb))+
  xlab("type") + ylab("engagement") 
```


Podemos ver como las fotos generan muchos mas actividad que cualquier otro tipo de publicacion. Los eventos, status y videos también generan bastantes likes/engagement, seguidos por los links y notas. En el próximo apartado nos centraremos sólo en los posts que representan un cambio en el Status.


******
# Análisis para los cambios de estado
******

Vamos a extraer un subconjunto de datos del fichero original,que contenga solo los registros con type=status y a realizar un análisis sobre estos. Clasificaremos nuestros datos en 3 grupos, en función del engagement, para establecer cuales son las palabras más usadas por grupo y ver si existen diferencias significativas.


```{r}

#Nos quedamo sólo con type=status
levels(datos_sent$type)
datos_status <-  datos_sent[which(datos_sent$type=="status"),]


# para este subconjunto vamos a crear una nueva variable categórica que tome los valores(alto, medio, bajo) en función del engagement

# primero ordenamos los datos de forma creciente, segun engagement
datos_status <- datos_status[order(datos_status$engagement_fb),] 

head(datos_status$engagement_fb)
tail(datos_status$engagement_fb)


#Calculalos los limites que dividiran la muestra en 3
limite <- quantile(datos_status$engagement_fb, c(0.33,0.66))

#Creamos una nueva variable (eng_level), definida como un factor, y caracterizada por tener el valor 'bajo' para el 33% mas bajo de engagement, 'medio' para valores entre el 33% y el 66%  y 'alto' para el resto.

datos_status$eng_level <- as.factor(ifelse(datos_status$engagement_fb<  limite[1],"bajo", ifelse(datos_status$engagement_fb <  limite[2],"medio","alto")))


#Calculamos la media de nº de engagement en función de la nueva variable
tapply(datos_status$engagement_fb,datos_status$eng_level,mean)


#Vamos a extraer el post_message de nuestros datos, separándolo en función del engagement level

linea_bajo <- datos_status$post_message[datos_status$eng_level=="bajo"]
linea_medio <- datos_status$post_message[datos_status$eng_level=="medio"]
linea_alto <- datos_status$post_message[datos_status$eng_level=="alto"]

linea_bajo = iconv(linea_bajo, to="ASCII//TRANSLIT")
linea_medio = iconv(linea_medio, to="ASCII//TRANSLIT")
linea_alto = iconv(linea_alto, to="ASCII//TRANSLIT")


# vamos a crear un corpus único con los documentos de las 3 clases (alta, media y baja). Lo utilizaremos para calcular el tfidf y después mostrar las nubes de palabras


lineas_all<- list(linea_bajo, linea_medio, linea_alto)
corpus_tfidf <- Corpus(VectorSource(lineas_all))

corpus_tfidf <- tm_map(corpus_tfidf, content_transformer(tolower)) 
# Quitamos la puntuacion
corpus_tfidf <- tm_map(corpus_tfidf, removePunctuation) 
# Quitamos numeros
corpus_tfidf <- tm_map(corpus_tfidf, removeNumbers)
# Quitamos espacios en blanco
corpus_tfidf <- tm_map(corpus_tfidf, stripWhitespace)
# Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, stopwords("spanish")) 
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, stopwords("english"))  
# Palabras especificas
# revisar, añadirlas a un fichero
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   
# sustituimos palabras derivadas 

# Indicamos que nuestro corpus es un texto
corpus_tfidf <- tm_map(corpus_tfidf, PlainTextDocument) 

# Creamos una matriz de terminos - documentos determinando que la frecuencia será tfidf
TDM_tfidf <-TermDocumentMatrix(corpus_tfidf,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

#reducimos la matriz de términos
TDM_tfidf<- removeSparseTerms(TDM_tfidf, 0.995)

inspect(TDM_tfidf)

TDM_tfidf_mat<- as.matrix(TDM_tfidf)

#creamos los df con las frecuencias de tfidf

colnames(TDM_tfidf_mat) <- c("freq_bajo", "freq_medio", "freq_alto")
word_tfidf_bajo <- data.frame(word=rownames(TDM_tfidf_mat), freq=TDM_tfidf_mat[,'freq_bajo'])
word_tfidf_medio <- data.frame(word=rownames(TDM_tfidf_mat), freq=TDM_tfidf_mat[,'freq_medio'])
word_tfidf_alto <- data.frame(word=rownames(TDM_tfidf_mat), freq=TDM_tfidf_mat[,'freq_alto'])


# histograma de frecuencias
#Añadimos una columna que nos indica la nueva categoria
df_freq_bajo <- data.frame(word_tfidf_bajo,"bajo")
df_freq_medio <- data.frame(word_tfidf_medio,"medio")
df_freq_alto <- data.frame(word_tfidf_alto,"alto")

colnames(df_freq_bajo)[3] <- "nivel"
colnames(df_freq_medio)[3] <- "nivel"
colnames(df_freq_alto)[3] <- "nivel"

union_df_freq <- data.frame(rbind(df_freq_bajo,df_freq_medio,df_freq_alto))

filtrado <- data.frame(subset(union_df_freq, freq>0.0035))

p <- ggplot(filtrado, aes(word, freq, fill=factor(nivel)))  
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p 
```


```{r}
#Vamos a crear 3 nubes de palabras, una para cada nivel, para visualizar las diferencias entre ellos.
#nubes de palabras

wordcloud(word_tfidf_bajo$word, word_tfidf_bajo$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)


wordcloud(word_tfidf_medio$word, word_tfidf_medio$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)


wordcloud(word_tfidf_alto$word, word_tfidf_alto$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)
```



******
# Análisis de polaridad y subjetividad
******


Filtramos para quedarnos solo con los registros en los que el post_message no sea vacío. Para dichos registros la polaridad y la subjetividad será cero y no queremos que influencie el análisis.

```{r}
datos_sent_fil <- datos_sent [datos_sent$post_message !="",]
```



Calculamos ahora las correlaciones entre la polaridad y las distintas variables

```{r}

pol_likes <- cor.test(datos_sent_fil$polarity, datos_sent_fil$likes_count_fb)

pol_commen <- cor.test(datos_sent_fil$polarity, datos_sent_fil$comments_count_fb)

pol_react <- cor.test(datos_sent_fil$polarity, datos_sent_fil$reactions_count_fb)

pol_share <- cor.test(datos_sent_fil$polarity, datos_sent_fil$shares_count_fb)

pol_engag <- cor.test(datos_sent_fil$polarity, datos_sent_fil$engagement_fb)

correlaciones <- c(pol_likes$estimate,pol_commen$estimate,pol_react$estimate,pol_share$estimate,pol_engag$estimate)

names(correlaciones) <- c("likes", "commen", "react","share","engage")

correlaciones

```

Realizamos lo mismo para la subjetividad

```{r}

sub_likes <- cor.test(datos_sent_fil$subjectivity, datos_sent_fil$likes_count_fb)

sub_commen <- cor.test(datos_sent_fil$subjectivity, datos_sent_fil$comments_count_fb)

sub_react <- cor.test(datos_sent_fil$subjectivity, datos_sent_fil$reactions_count_fb)

sub_share <- cor.test(datos_sent_fil$subjectivity, datos_sent_fil$shares_count_fb)

sub_engag <- cor.test(datos_sent_fil$subjectivity, datos_sent_fil$engagement_fb)

correlaciones_sub <- c(sub_likes$estimate,sub_commen$estimate,sub_react$estimate,sub_share$estimate,sub_engag$estimate)

names(correlaciones_sub) <- c("likes", "commen", "react","share","engage")

correlaciones_sub

```

```{r}
modelo_like <- lm(datos_sent_fil$polarity~datos_sent_fil$likes_count_fb)

summary(modelo_like)


modelo_comment <- lm(datos_sent_fil$polarity~datos_sent_fil$comments_count_fb)

summary(modelo_comment)



modelo_react <- lm(datos_sent_fil$polarity~datos_sent_fil$reactions_count_fb)

summary(modelo_react)


modelo_shares <- lm(datos_sent_fil$polarity~datos_sent_fil$shares_count_fb)

summary(modelo_shares)

modelo_engage <- lm(datos_sent_fil$polarity~datos_sent_fil$engagement_fb)

summary(modelo_engage)


```

Para los modelos con likes, reacciones y shares tenemos un p-valor muy bajo, lo que nos lleva a rechazar la hipótesis nula que nos indica que las variables no son estadísiticamente significativas.

Realizamos el mismo análisis para la subjetividad:



```{r}
modelo_like_sub <- lm(datos_sent_fil$subjectivity~datos_sent_fil$likes_count_fb)

summary(modelo_like_sub)


modelo_comment_sub <- lm(datos_sent_fil$subjectivity~datos_sent_fil$comments_count_fb)

summary(modelo_comment_sub)



modelo_react_sub <- lm(datos_sent_fil$subjectivity~datos_sent_fil$reactions_count_fb)

summary(modelo_react_sub)


modelo_shares_sub <- lm(datos_sent_fil$subjectivity~datos_sent_fil$shares_count_fb)

summary(modelo_shares_sub)

modelo_engage_sub <- lm(datos_sent_fil$subjectivity~datos_sent_fil$engagement_fb)

summary(modelo_engage_sub)


```

```{r}


#grafico que muestra la polaridad en función del nº de likes
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = polarity, y =   likes_count_fb))+
  xlab("polarity") + ylab("nº de likes") + geom_abline(colour="red")

#grafico que muestra el nº de comentarios en funcion del tipo de post
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = polarity, y =   comments_count_fb))+
  xlab("polarity") + ylab("nº de comentarios") + geom_abline(colour="red")

#grafico que muestra el nº de reacciones en funcion del tipo de post
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = polarity, y =   reactions_count_fb))+
  xlab("polarity") + ylab("nº de reacciones") + geom_abline(colour="red")

#grafico que muestra el nº de shares en funcion del tipo de post
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = polarity, y =   shares_count_fb))+
  xlab("polarity") + ylab("nº de shares") + geom_abline(colour="red")
#grafico que muestra el engagement en funcion del tipo de post
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = polarity, y =   engagement_fb))+
  xlab("polarity") + ylab("nº de engagement") + geom_abline(colour="red")



```
Hacemos lo mismo pero para la subjetividad

```{r}

#grafico que muestra la subjetividad en función del nº de likes
ggplot(data = datos_sent_fil) + 
   geom_point(mapping = aes(x = subjectivity, y =   likes_count_fb))+
  xlab("subjectivity") + ylab("nº de likes") + geom_abline(colour="red")

```




Continuamos con en análisis estadístico, ahora incorporando esta nueva variable que hemos definido y que contiene información sobre el engagement(alto,medio, bajo). Vamos a compararla con la polaridad.


```{r}
modelo_eng <- lm(polarity ~ eng_level, data=datos_status)
summary(modelo_eng)
```


```{r}
ggplot(data = datos_status) + 
   geom_boxplot(mapping = aes(x = eng_level, y =   polarity))+
  xlab("nivel engagement") + ylab("polaridad") +
   geom_hline(yintercept = 0, colour="red")


```

El p-valor es menor de 0.05, lo que nos indica que las variables son estadísticamente significativas.

Repetimos en el caso de la subjetividad:


```{r}
datos_status_fil1 <- datos_status[datos_status$subjectivity!=0,]

modelo_eng_fil1 <- lm(subjectivity ~ eng_level, data=datos_status_fil1)
summary(modelo_eng_fil1)

``` 


```{r}


ggplot(data = datos_status_fil1) + 
   geom_boxplot(mapping = aes(x = eng_level, y =   subjectivity))+
  xlab("nivel engagement") + ylab("subjetividad") +
  geom_hline(yintercept = 0.50, colour="red")
```


En el caso de la subjetividad el p-valor es de 0.09, por lo que no podemos concluir que sean estadísticamente significativas.


******
# Análisis variables temporales
******



Para terminar vamos a hacer uso de la fecha/hora en la que se realizó la intervención en Facebook para analizar si existe alguna relación entre las variables

```{r}
#Extraemos la fecha
datos_sent$fecha <- as.Date(fdatos$post_published_sql)

#Extraemos el dia de la semana
datos_sent$dia_sem <- weekdays(datos_sent$fecha)

#Extraemos la hora
datos_sent$hora <-  strftime(fdatos$post_published_sql, format='%H')


#Gráfico que muestra el engagement obtenido por día de la semana
ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = dia_sem, y =   engagement_fb))+
  xlab("dia semana") + ylab("engagement") 

#Gráfico que muestra el engagement obtenido por hora del dia
ggplot(data = datos_sent) + 
   geom_boxplot(mapping = aes(x = hora, y =   engagement_fb))+
  xlab("hora") + ylab("engagement")


```

Mediante esta visualización no podemos obtener demasiada información, busquemos mejor graficos que incluyan la frecuencia.

```{r}

#DF con la frecuencia por dia de la semena
day_freq <- as.data.frame(base::table(datos_sent$dia_sem))

#DF con la frecuencia por hora
hour_freq <- as.data.frame(base::table(datos_sent$hora))

#Ordenamos el factor para que al mostrar el gráfico aparezca ordenado de menor a mayor frecuencia
day_freq$Var1 <- factor(day_freq$Var1, levels = day_freq$Var1[order(day_freq$Freq)])


#Gráfico que muestra la frecuencia de mensajes según el día de la semana
p <- ggplot(day_freq, aes(Var1, Freq))  
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Dia") + ylab("Frecuencia")  
p 


```
Observamos que los lunes hay mayor nº de mensajes y los sábado es el día en el que menos hay. 

```{r}

#Gráfico que muestra la frecuencia de mensajes según la hora del día 
p <- ggplot(hour_freq, aes(Var1, Freq))  
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p <- p + xlab("Hora") + ylab("Frecuencia")
p
```
Según la hora del día también hay mucha variabilidad del nº de mensajes que se emiten.


Incluimos el tipo de post para visualizar la frecuencia por cada día de la semana.

```{r}


#DF con la frecuencia por dia de la semena y tipo de post
day_freq1 <- as.data.frame(base::table(datos_sent$dia_sem, datos_sent$type))


#Ordenamos el factor para que al mostrar el gráfico aparezca ordenado de menor a mayor frecuencia
day_freq1$Var1 <- factor(day_freq1$Var1, levels = day_freq1$Var1[order(day_freq$Freq)])


#Gráfico que muestra la frecuencia de mensajes según el día de la semana y el tipo de post
p <- ggplot(day_freq1, aes(Var1, Freq, fill=factor(Var2)))  
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) +xlab("Dia")+ylab("Frecuencia")
p$labels$fill <- "Tipo de Post" 
p 
```

Veamos si hay alguna relación entre la polaridad y los dias/horas de la semana en los que se emiten los mensajes.


En primer lugar vamos a transformar la variable hora para crear 4 intervalos de 6 horas cada uno, ya que será más sencillo de esta forma encontrar alguna relación.
```{r}
datos_sent$hora_int[(datos_sent$hora<="12") & (datos_sent$hora>"06") ] <- "mañana" 
datos_sent$hora_int[(datos_sent$hora<="18") & (datos_sent$hora>"12") ] <- "tarde" 
datos_sent$hora_int[(datos_sent$hora<="24") & (datos_sent$hora>"18") ] <- "noche"
datos_sent$hora_int[(datos_sent$hora<="06") & (datos_sent$hora>"00")] <- "madrugada"

datos_sent$hora_int<-as.factor(datos_sent$hora_int)
```



```{r}

#Realizamos un análisis de la varianza de la polaridad respecto a los días de la semana 

mod1 <- aov(datos_sent$polarity ~ datos_sent$dia_sem,datos_sent )

summary(mod1)


```
Obtenemos un p-valor muy alto lo que nos indica que las variables no son estadísticamente significativas

```{r}

#Modelo que precide la polaridad en función de la hora
mod2 <- aov(datos_sent$polarity ~ datos_sent$hora_int, datos_sent )

summary(mod2)



```
Obtenemos un p-valor muy pequeño lo que nos indica que las variables son estadísticamente significativas.


Realizamos el mismo estudio pero con el engagement.

```{r}

#Modelo que precide en engagement en función del día de la semana 
mod3 <- aov(datos_sent$engagement_fb ~ datos_sent$dia_sem,datos_sent )


summary(mod3)


```
Obtenemos un p-valor muy alto lo que nos indica que las variables no son estadísticamente significativas

```{r}

#Modelo que precide en engagement en función de la hora del día  
mod4 <- aov(datos_sent$engagement ~ datos_sent$hora_int, datos_sent )

summary(mod4)


```
Obtenemos un p-valor muy alto lo que nos indica que las variables no son estadísticamente significativas.


Vamos a calcular las medias de engagement y polaridad por dia de la semana y hora, para ver visualmente si hay alguna diferencia

```{r}
#Media de valores de engagement según el día de la semana
media_eng_dia <- tapply(datos_sent$engagement_fb, datos_sent$dia_sem, mean)

#Media de valores de engagement según la hora del día 
media_eng_hora <- tapply(datos_sent$engagement_fb, datos_sent$hora, mean)

#Media de la polaridad según el día de la semana
media_pol_dia <- tapply(datos_sent$polarity, datos_sent$dia_sem, mean)

#Media de valores de polaridad según la hora del día
media_pol_hora <- tapply(datos_sent$polarity, datos_sent$hora, mean)

#Convertimos en DF
media_eng_dia<-data.frame(media_eng_dia)
media_eng_hora<-data.frame(media_eng_hora)
media_pol_dia<-data.frame(media_pol_dia)
media_pol_hora<-data.frame(media_pol_hora)

#Grafico que muestra la media de engagement por día de la semana 
p <- ggplot(media_eng_dia, aes(names(media_eng_dia), media_eng_dia)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Dia") + ylab("Engagement medio")  
p

#Grafico que muestra la media de engagement por hora del día 
p <- ggplot(media_eng_hora, aes(names(media_eng_hora), media_eng_hora)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Hora")  +
ylab("Engagement medio")  
p

#Grafico que muestra la media de polaridad por día de la semana 
p <- ggplot(media_pol_dia, aes(names(media_pol_dia), media_pol_dia)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Dia")+
ylab("Polaridad media")  
p

#Grafico que muestra la media de polaridad por hora del día 
p <- ggplot(media_pol_hora, aes(names(media_pol_hora), media_pol_hora)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Hora")+
ylab("Polaridad media")  
p
```
Hacemos lo mismo pero considerando la hora ya agrupada en intervalos

```{r}

#Media de valores de engagement según la hora del día 
media_eng_hora_int <- tapply(datos_sent$engagement_fb, datos_sent$hora_int, mean)

#Media de valores de polaridad según la hora del día
media_pol_hora_int <- tapply(datos_sent$polarity, datos_sent$hora_int, mean)

#Convertimos en DF

media_eng_hora_int<-data.frame(media_eng_hora_int)
media_pol_hora_int<-data.frame(media_pol_hora_int)


#Grafico que muestra la media de polaridad por hora del día 
p <- ggplot(media_pol_hora_int, aes(names(media_pol_hora_int), media_pol_hora_int)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Intervalo Hora")+
ylab("Polaridad media")  
p

#Grafico que muestra la media de engagement por hora del día 
p <- ggplot(media_eng_hora_int, aes(names(media_eng_hora_int), media_eng_hora_int)  )
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab("Intervalo Hora")  +
ylab("Engagement medio")  
p


```

