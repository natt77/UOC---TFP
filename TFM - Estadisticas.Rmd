---
title: "TFM - Estadisticas"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



******
# Análisis por tipo de contenido
******

Leemos fichero de datos y seleccionamos columnas necesarias

```{r}

# Para la función Corpus()
library(tm)
# Para la función rbind.fill
library(plyr)
library(SnowballC)
# Para los graficos
library(ggplot2)  
# Para la nube de palabras
library(wordcloud)

#directorio de trabajo
nombreruta <-getwd()

#leemos todos los ficheros con datos de Facebook
fdatos <- data.frame()
file.names <- dir(nombreruta, pattern = "ANSII.txt")
for(i in 1:length(file.names)){
  file <- read.csv(file.names[i], header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII", sep="\t", fill=TRUE)
  fdatos <- rbind(fdatos, file)
}


#seleccionamos solo las columnas que queremos para las estadisticas
columnas <- c("type", "likes_count_fb", "comments_count_fb", "reactions_count_fb","shares_count_fb","engagement_fb")
datos <- data.frame(fdatos[,columnas])
head(datos,5)
summary(datos)


#transformamos las columnas para que sean numericos, salvo el type que sera un factor

datos$type <- as.factor(datos$type)
datos$likes_count_fb <- as.numeric(datos$likes_count_fb)
datos$comments_count_fb <- as.numeric(datos$comments_count_fb)
datos$reactions_count_fb <- as.numeric(datos$reactions_count_fb)
datos$shares_count_fb <- as.numeric(datos$shares_count_fb)
datos$engagement_fb <- as.numeric(datos$engagement_fb)


```

Vamos a calcular la media de cada uno de los contadores en función del tipo de contenido, y a mostrarlo visualmente

```{r}

#Calculamos las medias en funcion del type

tapply(datos$likes_count_fb, datos$type, mean)
tapply(datos$comments_count_fb, datos$type, mean)
tapply(datos$reactions_count_fb, datos$type, mean)
tapply(datos$shares_count_fb, datos$type, mean)
tapply(datos$engagement_fb, datos$type, mean)


#grafico que muestra el nº de likes en funcion del tipo de post
ggplot(data = datos) + 
   geom_point(mapping = aes(x = type, y = likes_count_fb))+
  xlab("type") + ylab("nº de likes")

#grafico que muestra el nº de comentarios en funcion del tipo de post
ggplot(data = datos) + 
   geom_point(mapping = aes(x = type, y = comments_count_fb))+
  xlab("type") + ylab("nº de comments")

#grafico que muestra el nº de reacciones en funcion del tipo de post
ggplot(data = datos) + 
   geom_point(mapping = aes(x = type, y = reactions_count_fb))+
  xlab("type") + ylab("nº de reacciones")

#grafico que muestra el nº de shares en funcion del tipo de post
ggplot(data = datos) + 
   geom_point(mapping = aes(x = type, y = shares_count_fb))+
  xlab("type") + ylab("nº de shares")

#grafico que muestra el engagement en funcion del tipo de post
ggplot(data = datos) + 
   geom_point(mapping = aes(x = type, y = engagement_fb))+
  xlab("type") + ylab("engagement")



```

Podemos ver como las fotos generan muchos mas likes/engagement que cualquier otro tipo de publicacion, seguido de cerca por el Status. En el próximo apartado nos centraremos sólo en los posts que representan un cambio en el Status.



******
# Análisis para los cambios de estado
******

Vamos a extraer un subconjunto de datos del fichero original, 
que contenga solo los registros con type=status y a realizar un análisis sobre estos. Clasificaremos nuestros datos en 3 grupos, en función del engagement, para establecer cuales son las palabras más usadas por grupo y ver si existen diferencias significativas.


```{r}

#Nos quedamo sólo con type=status
levels(datos$type)
datos_status <-  fdatos[which(fdatos$type=="status"),]


# para este subconjunto vamos a crear una nueva variable categórica que tome los valores(alto, medio, bajo) en función del engagement

# primero ordenamos los datos de forma creciente, segun engagement
datos_status <- datos_status[order(datos_status$engagement_fb),] 

head(datos_status$engagement_fb)
tail(datos_status$engagement_fb)

#Convertimos el engagement en una variable numercia
datos_status$engagement_fb <- as.numeric(datos_status$engagement_fb)

#Calculalos los limites que dividiran la muestra en 3
limite <- quantile(datos_status$engagement_fb, c(0.33,0.66))

#Creamos una nueva variable (eng_level), definida como un factor, y caracterizada por tener el valor 'bajo' para el 33% mas bajo de engagement, 'medio' para valores entre el 33% y el 66%  y 'alto' para el resto.

datos_status$eng_level <- as.factor(ifelse(datos_status$engagement_fb<  limite[1],"bajo", ifelse(datos_status$engagement_fb <  limite[2],"medio","alto")))


#Calculamos la media de nº de engagement en función de la nueva variable
tapply(datos_status$engagement_fb,datos_status$eng_level,mean)


#Vamos a extraer el post_message de nuestros datos, separándolo en función del engagement level

linea_bajo <- datos_status$post_message[datos_status$eng_level=="bajo"]
linea_medio <- datos_status$post_message[datos_status$eng_level=="medio"]
linea_alto <- datos_status$post_message[datos_status$eng_level=="alto"]

linea_bajo = iconv(linea_bajo, to="ASCII//TRANSLIT")
linea_medio = iconv(linea_medio, to="ASCII//TRANSLIT")
linea_alto = iconv(linea_alto, to="ASCII//TRANSLIT")


# Vamos a trabajar con un corpus, creamos primero una función que nos sirva para completar despues del stemming.
#stemCompletion2 <- function(x, dictionary){
#  x <- unlist(strsplit(as.character(x), " "))
#  x <- x[x != ""]
#  x <- stemCompletion(x, dictionary=dictionary)
#  x <- paste(x, sep="", collapse=" ")
#  PlainTextDocument(stripWhitespace(x))}


# Función que limpiará el corpus
fun_corpus <- function(x) {
  #creamos corpus
  doc.corpus <- Corpus(VectorSource(x))
  # Vamos a ir eliminando/modificando el corpus para quedarnos solo con     las palabras necesarias 
  # Transformamos a minÃºsculas
  doc.corpus <- tm_map(doc.corpus, content_transformer(tolower)) 
  # Quitamos la puntuaciÃ³n
  doc.corpus <- tm_map(doc.corpus, removePunctuation) 
  # Quitamos nÃºmeros
  doc.corpus <- tm_map(doc.corpus, removeNumbers)
  # Quitamos espacios en blanco
  doc.corpus <- tm_map(doc.corpus, stripWhitespace)
  # Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
  doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("spanish")) 
  doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))  
  # Palabras especificas
  # revisar, añadirlas a un fichero
  doc.corpus <- tm_map(doc.corpus, removeWords, c("vomitos","asociacion",   "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   
  # sustituimos palabras derivadas 
  # **** Esta parte de momento la dejamos comentada ya que necesita revisión
  # primero creamos una copia, que usaremos como diccionario
  #doc.corpus.copy <- doc.corpus 
  #sustituimos
  #doc.corpus <- tm_map(doc.corpus, stemDocument, language="spanish")
  
  #doc.corpus.new <- lapply(doc.corpus, stemCompletion2,    dictionary=doc.corpus.copy)
  
  #doc.corpus <- as.VCorpus(doc.corpus.new)
  
  # Indicamos que nuestro corpus es un texto
  doc.corpus <- tm_map(doc.corpus, PlainTextDocument) 
  
  # Creamos una matriz de terminos - documentos
  TDM <- TermDocumentMatrix(doc.corpus)
  
  
return(TDM)  
}


#Aplicamos a cada uno de los postmensajes la función para convertirlo en un corpus y limpiarlo
TDM_bajo <- fun_corpus(linea_bajo)
TDM_medio <- fun_corpus(linea_medio)
TDM_alto <- fun_corpus(linea_alto)

#Reducimos el nº de ceros de la matriz de términos
TDM_bajo <- removeSparseTerms(TDM_bajo, 0.99)
TDM_medio <- removeSparseTerms(TDM_medio, 0.99)
TDM_alto <- removeSparseTerms(TDM_alto, 0.99)


#Definimos función

fun_matriz <- function(x){
TDM_matrix<-as.matrix(x)
frecuencia <- sort(rowSums(TDM_matrix), decreasing=TRUE)
palab_frec.df <- data.frame(word=names(frecuencia), freq=frecuencia)

return(palab_frec.df)
}

df_freq_bajo <- fun_matriz(TDM_bajo)
df_freq_medio <- fun_matriz(TDM_medio)
df_freq_alto <- fun_matriz(TDM_alto)

#Añadimos una columna que nos indica la nueva categoria
df_freq_bajo <- data.frame(df_freq_bajo,"bajo")
df_freq_medio <- data.frame(df_freq_medio,"medio")
df_freq_alto <- data.frame(df_freq_alto,"alto")

colnames(df_freq_bajo)[3] <- "nivel"
colnames(df_freq_medio)[3] <- "nivel"
colnames(df_freq_alto)[3] <- "nivel"

union_df_freq <- data.frame(rbind(df_freq_bajo,df_freq_medio,df_freq_alto))

                            
filtrado <- data.frame(subset(union_df_freq, freq>40))

p <- ggplot(filtrado, aes(word, freq, fill=factor(nivel)))  
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p 
```
Ahora vamos a crear 2 nubes de palabras, una para lo de nivel bajo y otra para los de nivel alto, para visualizar las diferencias entre ellos.

```{r}

#nube de palabras nivel alto
wordcloud(df_freq_alto$word, df_freq_alto$freq, random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)

#nube de palabras nivel bajo
wordcloud(df_freq_bajo$word, df_freq_bajo$freq, random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)

```
