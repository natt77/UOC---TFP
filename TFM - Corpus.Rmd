---
title: "TFM-Corpus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
#Cargamos librerías y leemos ficheros Facebook
******


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Para la funcion Corpus()
library(tm)
# Para la funciÃ³n rbind.fill
library(plyr)
library(SnowballC)
# Para los graficos
library(ggplot2)  
# Para la nube de palabras
library(wordcloud)

#directorio de trabajo
nombreruta <-getwd()

#leemos todos los ficheros con datos de Facebook
fdatos <- data.frame()
file.names <- dir(nombreruta, pattern = "ANSII.txt")
for(i in 1:length(file.names)){
  file <- read.table(file.names[i], header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII", sep="\t", fill=TRUE)
  fdatos <- rbind(fdatos, file)
}


#Extraemos solo la variable que contiene el post_mensaje
linea <- fdatos$post_message

#lo convertimos para eliminar tildes 

linea = iconv(linea, to="ASCII//TRANSLIT")


```


******
#Creación del corpus de Facebook
******

```{r}
#creamos corpus
doc.corpus <- Corpus(VectorSource(linea))

# Vamos a ir eliminando/modificando el corpus para quedarnos solo con las palabras necesarias 
# Transformamos a minÃºsculas
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower)) 
# Quitamos la puntuacion
doc.corpus <- tm_map(doc.corpus, removePunctuation) 
# Quitamos numeros
doc.corpus <- tm_map(doc.corpus, removeNumbers)
# Quitamos espacios en blanco
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("spanish")) 
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))  
# Palabras especificas
# revisar, añadirlas a un fichero
doc.corpus <- tm_map(doc.corpus, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   
# sustituimos palabras derivadas 
# **** Esta parte de momento la dejamos comentada ya que necesita revisión
# primero creamos una copia, que usaremos como diccionario
#doc.corpus.copy <- doc.corpus 
#sustituimos
#doc.corpus <- tm_map(doc.corpus, stemDocument, language="spanish")

#creamoos una función para completar
#stemCompletion2 <- function(x, dictionary){
#  x <- unlist(strsplit(as.character(x), " "))
  # Esto es para que los espacios no se sustituyan por palabras
#  x <- x[x != ""]
#  x <- stemCompletion(x, dictionary=dictionary)
#  x <- paste(x, sep="", collapse=" ")
#  PlainTextDocument(stripWhitespace(x))}

#Llamamos a la funcion para completar
#doc.corpus <- lapply(doc.corpus, stemCompletion2, #dictionary=doc.corpus.copy)
#***************************
# Transformamos de lista a corpus 
#doc.corpus <- as.VCorpus(doc.corpus)

# Indicamos que nuestro corpus es un texto
doc.corpus <- tm_map(doc.corpus, PlainTextDocument) 

# Creamos una matriz de terminos - documentos
TDM <- TermDocumentMatrix(doc.corpus)

# Para evitar tener palabras que son muy cortas 
# (2,inf) nos indica la longitud minima de las palabras, por defecto es 3
TDM <- TermDocumentMatrix(doc.corpus, 
       control = list(wordLengths = c(3, Inf))) 

# Veamos que tamaño tiene
dim(TDM)

inspect(TDM[1:20,1:8])



# Reducimos la matriz
# cuanto mayor ponemos el coeficiente mÃ¡s palabras tenemos
# probar con varios valores
TDM <- removeSparseTerms(TDM, 0.995)
#dtms
inspect(TDM[1:5,1:5])



#muestra matriz de terminos
TDM_matrix<-as.matrix(TDM)
frecuencia <- sort(rowSums(TDM_matrix), decreasing=TRUE)
palab_frec.df <- data.frame(word=names(frecuencia), freq=frecuencia)


```


******
# Visualizaciones datos Facebook
******
```{r}

#visualizamos una grafica con la frecuenca de las palabras

#Seleccionamos solo las que aparecen mas de 150 veces
#probar varios valores
filtrado <- data.frame(subset(palab_frec.df, freq>150))

# histograma de frecuencias
p <- ggplot(filtrado, aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p 


#nube de palabras
wordcloud(palab_frec.df$word, palab_frec.df$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)
```

******
#Creación del corpus del Decálogo
******


```{r}

#Vamos a leer el decálogo de prioridades, y de forma análogo a lo que hemos hecho para los datos de Facebook crearemos un corpus y una nube de palabras

decalogo <- read.table("decalogodeprioridades.txt", header = FALSE, stringsAsFactors = FALSE,encoding = "UTF-8", sep="\t" )

#creamos corpus
doc.corpus.dec <- Corpus(VectorSource(decalogo))

# Vamos a ir eliminando/modificando el corpus para quedarnos sÃ³lo con las palabras necesarias 
# Transformamos a minusculas
doc.corpus.dec <- tm_map(doc.corpus.dec, content_transformer(tolower)) 
# Quitamos la puntuacionn
doc.corpus.dec <- tm_map(doc.corpus.dec, removePunctuation) 
# Quitamos nÃºmeros
doc.corpus.dec <- tm_map(doc.corpus.dec, removeNumbers)
# Quitamos espacios en blanco
doc.corpus.dec <- tm_map(doc.corpus.dec, stripWhitespace)
# Quitamos palabras sin valor analitico, en español
doc.corpus.dec <- tm_map(doc.corpus.dec, removeWords, stopwords("spanish")) 
# Palabras especificas
# revisar, añadirlas a un fichero
doc.corpus <- tm_map(doc.corpus, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   

# sustituimos palabras derivadas 
# **** Esta parte de momento la dejamos comentada ya que necesita revisión
# primero creamos una copia, que usaremos como diccionario
#doc.corpus.copy <- doc.corpus.dec 
# sustituimos
#doc.corpus.dec <- tm_map(doc.corpus.dec, stemDocument, language="spanish")

#Llamamos a la funcion para completar
#doc.corpus.dec <- lapply(doc.corpus.dec, stemCompletion2, dictionary=doc.corpus.copy)
# Transformamos de lista a corpus 
#doc.corpus.dec <- as.VCorpus(doc.corpus.dec)

# Indicamos que nuestro corpus es un texto
doc.corpus.dec <- tm_map(doc.corpus.dec, PlainTextDocument) 


# Creamos una matriz de terminos - documentos
TDM.dec <- TermDocumentMatrix(doc.corpus.dec)


# Para evitar tener palabras que son muy cortas 
# (2,inf) nos indica la longitud minima de las palabras, por defecto es 3
TDM.dec <- TermDocumentMatrix(doc.corpus.dec, 
       control = list(wordLengths = c(3, Inf))) 

# Veamos que tamaño tiene
dim(TDM.dec)


#muestra matriz de terminos
TDM_matrix_dec<-as.matrix(TDM.dec)
frecuencia_dec <- sort(rowSums(TDM_matrix_dec), decreasing=TRUE)
palab_frec_dec.df <- data.frame(word=names(frecuencia_dec), freq=frecuencia_dec)

```

******
# Visualizaciones decálogo
******
```{r}

#visualizamos una grafica con la frecuenca de las palabras

#Seleccionamos sólo las que aparecen mas de 4 veces
#probar varios valores
filtrado_dec <- data.frame(subset(palab_frec_dec.df, freq>4))

# histograma de frecuencias
p1 <- ggplot(filtrado_dec, aes(word, freq))    
p1 <- p1 + geom_bar(stat="identity")   
p1 <- p1 + theme(axis.text.x=element_text(angle=45, hjust=1))   
p1 


#nube de palabras

wordcloud(palab_frec_dec.df$word, palab_frec_dec.df$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)



```

******
# Comparativa entre palabras 
******

Vamos a calcular el log-likelihood (LL) ratio de cada palabra y mostrarlo ordenado. Las primeras que aparezcan nos indicarán mayores diferencias de ratio, es decir, mayor diferencia relativa entre las apariciones entre ambos textos. De forma similar mostraremos las últimas en aparecer, que mostraran las que tiene ratios similares.

```{r}


#Partimos de los 2 df que teníamos con las palabras y frecuencias de Facebook (palab_frec) y del decálogo (palab_frec_dec)

#renombramos las variables de frecuencia para que sean distintas

names(palab_frec.df)[2] <- "frec1"
names(palab_frec_dec.df)[2] <- "frec2"

#Obtenemos solo las palabras comunes a los dos ficheros
unido <- join(palab_frec.df, palab_frec_dec.df, by="word" , type="inner")

#calculamos el numero total de palabras de cada corpus
c <- sum(unido$frec1)
d <- sum(unido$frec2)

#frecuencias de cada palabra en cada corpus
a <- unido$frec1
b <- unido$frec2

#Realizamos los cálculos finales
E1=c*(a+b)/(c+d)
E2=d*(a+b)/(c+d)

unido$LL <- 2 * ( a*log(a/E1) + b*log(b/E2) )

#Ordenamos
unido <- as.data.frame(unido)
unido <- unido[order(-unido[,4]),]

#Mostramos las palabras con un LL más alto
head(unido)

#Mostramos las palabras con un LL más bajo
tail(unido)

```
******
# Comparativa entre nubes de palabras 
******

```{r}
library(quanteda)

palab_frec.df$tipo <- "Facebook"
palab_frec_dec.df$tipo <- "Decalogo"

unido_all <- join(palab_frec.df, palab_frec_dec.df, by="word" , type="full")

unido_all$frec1<-ifelse(is.na(unido_all$frec1),0, unido_all$frec1)
unido_all$frec2<-ifelse(is.na(unido_all$frec2),0, unido_all$frec2)

#unido_all$frec<- unido_all$frec1 + unido_all$frec2

unido_all <- unido_all[,c(1,2,4)]
unido_all_mat <- as.matrix(unido_all)

row.names(unido_all_mat) <- unido_all_mat[,1]

unido_all_mat <- unido_all_mat[,c(2,3)]
unido_all_df <- as.data.frame(unido_all_mat)

unido_all_df$frec1 <- as.numeric(unido_all_df$frec1)
unido_all_df$frec2 <- as.numeric(unido_all_df$frec2)

names(unido_all_df)[1] <- "Facebook"
names(unido_all_df)[2] <- "Decalogo"

unido_all_mat <- as.matrix(unido_all_df)

comparison.cloud(unido_all_mat,scale=c(0.5,1.5),max.words=80,
	random.order=FALSE,rot.per=0,
	colors=brewer.pal(3,"Dark2"),
	use.r.layout=TRUE,title.size=1)

```
