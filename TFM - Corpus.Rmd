---
title: "TFM-Corpus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
#Cargamos librerías y leemos ficheros
******


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Para la funciÃ³n Corpus()
library(tm)
# Para la funciÃ³n rbind.fill
library(plyr)
library(SnowballC)
# Para los graficos
library(ggplot2)  
# Para la nube de palabras
library(wordcloud)

#directorio de trabajo
nombreruta <-getwd()

#leemos todos los ficheros con datos de Facebook
fdatos <- data.frame()
file.names <- dir(nombreruta, pattern = ".csv")
for(i in 1:length(file.names)){
  file <- read.csv(file.names[i], header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII")
  fdatos <- rbind(fdatos, file)
}


#Extraemos solo la variable que contiene el post_mensaje
linea <- fdatos$post_message

#lo convertimos para eliminar tildes 
#revisar cómo hacerlo
#linea = iconv(linea, to="ASCII//TRANSLIT")


```


******
#Creación del corpus
******

```{r}
#creamos corpus
doc.corpus <- Corpus(VectorSource(linea))

# Vamos a ir eliminando/modificando el corpus para quedarnos sÃ³lo con las palabras necesarias 
# Transformamos a minÃºsculas
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower)) 
# Quitamos la puntuaciÃ³n
doc.corpus <- tm_map(doc.corpus, removePunctuation) 
# Quitamos nÃºmeros
doc.corpus <- tm_map(doc.corpus, removeNumbers)
# Quitamos espacios en blanco
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("spanish")) 
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))  
# Palabras especificas
*# revisar, añadirlas a un fichero
doc.corpus <- tm_map(doc.corpus, removeWords, c("vomito", "enfermedad","sindrome","asociacion","marfan","paramo","gracias"))   
# sustituimos palabras derivadas 
# OJO --> por ejemplo abrazo pone abraz, ver cómo queda al final y si nos interesa hacerlo
doc.corpus <- tm_map(doc.corpus, stemDocument, language="spanish")


# Indicamos que nuestro corpus es un texto
doc.corpus <- tm_map(doc.corpus, PlainTextDocument) 

*
# Creamos una matriz de terminos - documentos
TDM <- TermDocumentMatrix(doc.corpus)



# Para evitar tener palabras que son muy cortas 
# (2,inf) nos indica la longitud minima de las palabras, por defecto es 3
TDM <- TermDocumentMatrix(doc.corpus, 
       control = list(wordLengths = c(3, Inf))) 
*
# Veamos que tamaño tiene
dim(TDM)

inspect(TDM[1:20,1:8])



# Reducimos la matriz
# cuanto mayor ponemos el coeficiente mÃ¡s palabras tenemos
# probar con varios valores
TDM <- removeSparseTerms(TDM, 0.99)
#dtms
inspect(TDM[1:5,1:5])



#muestra matriz de terminos
TDM_matrix<-as.matrix(TDM)
frecuencia <- sort(rowSums(TDM_matrix), decreasing=TRUE)
palab_frec.df <- data.frame(word=names(frecuencia), freq=frecuencia)


```


******
# Visualizaciones
******
```{r}

#visualizamos una grafica con la frecuenca de las palabras

#Seleccionamos sÃ³lo las que aparecen mas de 200 veces
#probar varios valores
filtrado <- data.frame(subset(palab_frec.df, freq>150))

# histograma de frecuencias
p <- ggplot(filtrado, aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p 


#nube de palabras

wordcloud(palab_frec.df$word, palab_frec.df$freq, min.freq=150, random.color=TRUE, colors=rainbow(7))


```
******
#correlaciones entre palabras
******
```{r}
findAssocs(TDM, c("asociacion"), corlimit=0.19)
```
 
