---
title: "TFM-Corpus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
#Cargamos librerías y leemos ficheros Facebook
******


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Para la funcion Corpus()
library(tm)
# Para la funciÃ³n rbind.fill
library(plyr)
library(SnowballC)
# Para los graficos
library(ggplot2)  
# Para la nube de palabras
library(wordcloud)

#directorio de trabajo
nombreruta <-getwd()

#leemos todos los ficheros con datos de Facebook
fdatos <- data.frame()
file.names <- dir(nombreruta, pattern = "ANSII.txt")
for(i in 1:length(file.names)){
  file <- read.table(file.names[i], header = TRUE, stringsAsFactors = FALSE,encoding = "ANSII", sep="\t", fill=TRUE)
  fdatos <- rbind(fdatos, file)
}


#Extraemos solo la variable que contiene el post_mensaje
linea <- fdatos$post_message

#lo convertimos para eliminar tildes 

linea = iconv(linea, to="ASCII//TRANSLIT")


```


******
#Creación del corpus de Facebook
******

```{r}
#creamos corpus
doc.corpus <- Corpus(VectorSource(linea))

# Vamos a ir eliminando/modificando el corpus para quedarnos solo con las palabras necesarias 
# Transformamos a minÃºsculas
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower)) 
# Quitamos la puntuacion
doc.corpus <- tm_map(doc.corpus, removePunctuation) 
# Quitamos numeros
doc.corpus <- tm_map(doc.corpus, removeNumbers)
# Quitamos espacios en blanco
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("spanish")) 
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))  
# Palabras especificas
# revisar, añadirlas a un fichero
doc.corpus <- tm_map(doc.corpus, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   
# sustituimos palabras derivadas 

# Indicamos que nuestro corpus es un texto
doc.corpus <- tm_map(doc.corpus, PlainTextDocument) 

# Creamos una matriz de terminos - documentos
TDM <- TermDocumentMatrix(doc.corpus)

# Para evitar tener palabras que son muy cortas 
# (2,inf) nos indica la longitud minima de las palabras, por defecto es 3
TDM <- TermDocumentMatrix(doc.corpus, 
       control = list(wordLengths = c(3, Inf))) 

# Veamos que tamaño tiene
dim(TDM)

inspect(TDM[1:20,1:8])



# Reducimos la matriz
# cuanto mayor ponemos el coeficiente mÃ¡s palabras tenemos
# probar con varios valores
TDM <- removeSparseTerms(TDM, 0.995)
#dtms
inspect(TDM[1:5,1:5])



#muestra matriz de terminos
TDM_matrix<-as.matrix(TDM)
frecuencia <- sort(rowSums(TDM_matrix), decreasing=TRUE)
palab_frec.df <- data.frame(word=names(frecuencia), freq=frecuencia)


```


******
# Visualizaciones datos Facebook
******
```{r}

#visualizamos una grafica con la frecuenca de las palabras

#Seleccionamos solo las que aparecen mas de 150 veces
#probar varios valores
filtrado <- data.frame(subset(palab_frec.df, freq>150))

# histograma de frecuencias
p <- ggplot(filtrado, aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p 


#nube de palabras
wordcloud(palab_frec.df$word, palab_frec.df$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)
```

******
#Creación del corpus del Decálogo
******


```{r}

#Vamos a leer el decálogo de prioridades, y de forma análogo a lo que hemos hecho para los datos de Facebook crearemos un corpus y una nube de palabras

decalogo <- read.table("decalogodeprioridades.txt", header = FALSE, stringsAsFactors = FALSE,encoding = "UTF-8", sep="\t" )

#creamos corpus
doc.corpus.dec <- Corpus(VectorSource(decalogo))

# Vamos a ir eliminando/modificando el corpus para quedarnos sÃ³lo con las palabras necesarias 
# Transformamos a minusculas
doc.corpus.dec <- tm_map(doc.corpus.dec, content_transformer(tolower)) 
# Quitamos la puntuacionn
doc.corpus.dec <- tm_map(doc.corpus.dec, removePunctuation) 
# Quitamos nÃºmeros
doc.corpus.dec <- tm_map(doc.corpus.dec, removeNumbers)
# Quitamos espacios en blanco
doc.corpus.dec <- tm_map(doc.corpus.dec, stripWhitespace)
# Quitamos palabras sin valor analitico, en español
doc.corpus.dec <- tm_map(doc.corpus.dec, removeWords, stopwords("spanish")) 
# Palabras especificas
# revisar, añadirlas a un fichero
doc.corpus.dec <- tm_map(doc.corpus.dec, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   


# Indicamos que nuestro corpus es un texto
doc.corpus.dec <- tm_map(doc.corpus.dec, PlainTextDocument) 


# Creamos una matriz de terminos - documentos
TDM.dec <- TermDocumentMatrix(doc.corpus.dec)


# Para evitar tener palabras que son muy cortas 
# (2,inf) nos indica la longitud minima de las palabras, por defecto es 3
TDM.dec <- TermDocumentMatrix(doc.corpus.dec, 
       control = list(wordLengths = c(3, Inf))) 

# Veamos que tamaño tiene
dim(TDM.dec)


#muestra matriz de terminos
TDM_matrix_dec<-as.matrix(TDM.dec)
frecuencia_dec <- sort(rowSums(TDM_matrix_dec), decreasing=TRUE)
palab_frec_dec.df <- data.frame(word=names(frecuencia_dec), freq=frecuencia_dec)

```

******
# Visualizaciones decálogo
******
```{r}

#visualizamos una grafica con la frecuenca de las palabras

#Seleccionamos sólo las que aparecen mas de 4 veces
#probar varios valores
filtrado_dec <- data.frame(subset(palab_frec_dec.df, freq>4))

# histograma de frecuencias
p1 <- ggplot(filtrado_dec, aes(word, freq))    
p1 <- p1 + geom_bar(stat="identity")   
p1 <- p1 + theme(axis.text.x=element_text(angle=45, hjust=1))   
p1 


#nube de palabras

wordcloud(palab_frec_dec.df$word, palab_frec_dec.df$freq, scale=c(3,0.5), random.color=FALSE,random.order = FALSE, colors=colorRampPalette(brewer.pal(6,"Blues"))(32),
max.words=45, rot.per=0)



```

******
# Comparativa entre palabras 
******

Vamos a calcular el log-likelihood (LL) ratio de cada palabra y mostrarlo ordenado. Las primeras que aparezcan nos indicarán mayores diferencias de ratio, es decir, mayor diferencia relativa entre las apariciones entre ambos textos. De forma similar mostraremos las últimas en aparecer, que mostraran las que tiene ratios similares.

```{r}


#Partimos de los 2 df que teníamos con las palabras y frecuencias de Facebook (palab_frec) y del decálogo (palab_frec_dec)

#renombramos las variables de frecuencia para que sean distintas

names(palab_frec.df)[2] <- "frec1"
names(palab_frec_dec.df)[2] <- "frec2"

#Obtenemos solo las palabras comunes a los dos ficheros
unido <- join(palab_frec.df, palab_frec_dec.df, by="word" , type="inner")

#calculamos el numero total de palabras de cada corpus
c <- sum(unido$frec1)
d <- sum(unido$frec2)

#frecuencias de cada palabra en cada corpus
a <- unido$frec1
b <- unido$frec2

#Realizamos los cálculos finales
E1=c*(a+b)/(c+d)
E2=d*(a+b)/(c+d)

unido$LL <- 2 * ( a*log(a/E1) + b*log(b/E2) )

#Ordenamos
unido <- as.data.frame(unido)
unido <- unido[order(-unido[,4]),]

#Mostramos las palabras con un LL más alto
head(unido)

#Mostramos las palabras con un LL más bajo
tail(unido)

```
******
# Comparativa entre nubes de palabras 
******

```{r}
library(quanteda)

palab_frec.df$tipo <- "Facebook"
palab_frec_dec.df$tipo <- "Decalogo"

unido_all <- join(palab_frec.df, palab_frec_dec.df, by="word" , type="full")

unido_all$frec1<-ifelse(is.na(unido_all$frec1),0, unido_all$frec1)
unido_all$frec2<-ifelse(is.na(unido_all$frec2),0, unido_all$frec2)

#unido_all$frec<- unido_all$frec1 + unido_all$frec2

unido_all <- unido_all[,c(1,2,4)]
unido_all_mat <- as.matrix(unido_all)

row.names(unido_all_mat) <- unido_all_mat[,1]

unido_all_mat <- unido_all_mat[,c(2,3)]
unido_all_df <- as.data.frame(unido_all_mat)

unido_all_df$frec1 <- as.numeric(unido_all_df$frec1)
unido_all_df$frec2 <- as.numeric(unido_all_df$frec2)

names(unido_all_df)[1] <- "Facebook"
names(unido_all_df)[2] <- "Decalogo"

unido_all_mat <- as.matrix(unido_all_df)

comparison.cloud(unido_all_mat,scale=c(0.5,2.0),max.words=80,
	random.order=FALSE,rot.per=0,
	colors=brewer.pal(3,"Dark2"),
	use.r.layout=TRUE,title.size=1)

```
*******
#Calculo tf-idf 
*******
```{r}
#leemos datos de facebook en una única variable
facebook<-" "
for(i in 1:length(linea)){
  fila <- linea[i]
  facebook<-paste(facebook,fila)
}

all<-c(facebook,decalogo)

#creamos corpus con 2 documentos, una para facebook y otro para el decálogo
corpus_tfidf <- Corpus(VectorSource(all))

corpus_tfidf <- tm_map(corpus_tfidf, content_transformer(tolower)) 
# Quitamos la puntuacion
corpus_tfidf <- tm_map(corpus_tfidf, removePunctuation) 
# Quitamos numeros
corpus_tfidf <- tm_map(corpus_tfidf, removeNumbers)
# Quitamos espacios en blanco
corpus_tfidf <- tm_map(corpus_tfidf, stripWhitespace)
# Quitamos palabras sin valor analitico, en ingles y espaÃ±ol
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, stopwords("spanish")) 
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, stopwords("english"))  
# Palabras especificas
# revisar, añadirlas a un fichero
corpus_tfidf <- tm_map(corpus_tfidf, removeWords, c("vomitos","asociacion", "enfermedad","sindrome","marfan","moebius","mas","asi","raras","wolfram"))   
# sustituimos palabras derivadas 

# Indicamos que nuestro corpus es un texto
corpus_tfidf <- tm_map(corpus_tfidf, PlainTextDocument) 

# Creamos una matriz de terminos - documentos determinando que la frecuencia será tfidf
TDM_tfidf <-TermDocumentMatrix(corpus_tfidf,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

#reducimos la matriz de términos
TDM_tfidf <- removeSparseTerms(TDM_tfidf, 0.995)

TDM_tfidf<-as.matrix(TDM_tfidf)
freq_tfidf <-  sort(rowSums(TDM_tfidf), decreasing=TRUE)

#creamos df con las palabras y su frecuencia
palab_frec_tfidf <- data.frame(word=names(freq_tfidf), freq=freq_tfidf)

head(palab_frec_tfidf,10)

tail(palab_frec_tfidf,10)
```

```{r}
#voy a hacer algo parecido pero solo con las palabras comunes a ambos documentos para ver los resultados

colnames(TDM_tfidf) <- c("tfidf1", "tfidf2")
TDM_tfidf_nozero <- TDM_tfidf[(TDM_tfidf[,1]>0&TDM_tfidf[,2]>0) ,]

#Hay algo que no me cuadra, no aparecn palabras comunes.
```
